{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85YKqyjdrRtD"
      },
      "source": [
        "###Download BTHOWeN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8WtcTi1qxj5",
        "outputId": "214da287-cfcf-40fd-9178-1084871ca8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BTHOWeN'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 104 (delta 44), reused 94 (delta 34), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (104/104), 1.65 MiB | 13.61 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ZSusskind/BTHOWeN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "497mRSKOrFVO",
        "outputId": "7bc482ad-9e59-4d1d-ee51-e2a4ddf5ee87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA-zmHLMo4ZR"
      },
      "source": [
        "###Reescrevendo train_swept_models para nosso dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUJWGnQto9r7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "collapsed": true,
        "outputId": "aa562276-f2c9-4381-959c-9c02bac765d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wisard'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-635f66045308>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwisard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWiSARD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wisard'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "import itertools\n",
        "import argparse\n",
        "import ctypes as c\n",
        "import numpy as np\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from scipy.stats import norm\n",
        "\n",
        "# For saving models\n",
        "import pickle\n",
        "import lzma\n",
        "import os\n",
        "\n",
        "from wisard import WiSARD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuXyr3wZpJsy"
      },
      "outputs": [],
      "source": [
        "# Perform inference operations using provided test set on provided model with specified bleaching value (default 1)\n",
        "def run_inference(inputs, labels, model, bleach=1):\n",
        "    num_samples = len(inputs)\n",
        "    correct = 0\n",
        "    ties = 0\n",
        "    model.set_bleaching(bleach)\n",
        "    for d in range(num_samples):\n",
        "        prediction = model.predict(inputs[d])\n",
        "        label = labels[d]\n",
        "        if len(prediction) > 1:\n",
        "            ties += 1\n",
        "        if prediction[0] == label:\n",
        "            correct += 1\n",
        "    correct_percent = round((100 * correct) / num_samples, 4)\n",
        "    tie_percent = round((100 * ties) / num_samples, 4)\n",
        "    print(f\"With bleaching={bleach}, accuracy={correct}/{num_samples} ({correct_percent}%); ties={ties}/{num_samples} ({tie_percent}%)\")\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeT14NSEp8Of"
      },
      "outputs": [],
      "source": [
        "def parameterized_run(train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels, unit_inputs, unit_entries, unit_hashes):\n",
        "    model = WiSARD(train_inputs[0].size, train_labels.max()+1, unit_inputs, unit_entries, unit_hashes)\n",
        "\n",
        "    print(\"Training model\")\n",
        "    for d in range(len(train_inputs)):\n",
        "        model.train(train_inputs[d], train_labels[d])\n",
        "        if ((d+1) % 10000) == 0:\n",
        "            print(d+1)\n",
        "\n",
        "    max_val = 0\n",
        "    for d in model.discriminators:\n",
        "        for f in d.filters:\n",
        "            max_val = max(max_val, f.data.max())\n",
        "    print(f\"Maximum possible bleach value is {max_val}\")\n",
        "    # Use a binary search-based strategy to find the value of b that maximizes accuracy on the validation set\n",
        "    best_bleach = max_val // 2\n",
        "    step = max(max_val // 4, 1)\n",
        "    bleach_accuracies = {}\n",
        "    while True:\n",
        "        values = [best_bleach-step, best_bleach, best_bleach+step]\n",
        "        accuracies = []\n",
        "        for b in values:\n",
        "            if b in bleach_accuracies:\n",
        "                accuracies.append(bleach_accuracies[b])\n",
        "            elif b < 1:\n",
        "                accuracies.append(0)\n",
        "            else:\n",
        "                accuracy = run_inference(val_inputs, val_labels, model, b)\n",
        "                bleach_accuracies[b] = accuracy\n",
        "                accuracies.append(accuracy)\n",
        "        new_best_bleach = values[accuracies.index(max(accuracies))]\n",
        "        if (new_best_bleach == best_bleach) and (step == 1):\n",
        "            break\n",
        "        best_bleach = new_best_bleach\n",
        "        if step > 1:\n",
        "            step //= 2\n",
        "    print(f\"Best bleach: {best_bleach}; inputs/entries/hashes = {unit_inputs},{unit_entries},{unit_hashes}\")\n",
        "    # Evaluate on test set\n",
        "    print(\"Testing model\")\n",
        "    accuracy = run_inference(test_inputs, test_labels, model, bleach=best_bleach)\n",
        "    return model, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAogWgg23bcl"
      },
      "outputs": [],
      "source": [
        "def get_datasets_from_folders(base_dir):\n",
        "    \"\"\"\n",
        "    Carrega um dataset onde as imagens estão organizadas em pastas por classe.\n",
        "    \"\"\"\n",
        "    train_dataset = []\n",
        "    test_dataset = []\n",
        "    valid_dataset = []\n",
        "\n",
        "    for split in ['train', 'test', 'valid']:  # Inclui 'valid' como um split adicional\n",
        "        split_path = os.path.join(base_dir, split)\n",
        "        if not os.path.exists(split_path):\n",
        "            if split == 'valid':  # Validação é opcional\n",
        "                print(f\"Aviso: Split de validação {split_path} não encontrado. Pulando.\")\n",
        "                continue\n",
        "            else:\n",
        "                raise ValueError(f\"Split path {split_path} não encontrado.\")\n",
        "\n",
        "        for class_name in os.listdir(split_path):\n",
        "            class_path = os.path.join(split_path, class_name)\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue  # Ignorar arquivos\n",
        "\n",
        "            class_label = int(class_name)  # Assumindo que o nome da pasta é o índice da classe\n",
        "            for img_file in os.listdir(class_path):\n",
        "                img_path = os.path.join(class_path, img_file)\n",
        "                try:\n",
        "                    # Abrir imagem e realizar pré-processamento\n",
        "                    img = Image.open(img_path).convert(\"L\").resize((160, 160))\n",
        "                    img_array = np.array(img)  # Escala de cinza\n",
        "                    binarized_img = (img_array >= img_array.mean()).astype(np.uint8).flatten()  # Binarização\n",
        "                    data_tuple = (binarized_img, class_label)\n",
        "\n",
        "                    if split == 'train':\n",
        "                        train_dataset.append(data_tuple)\n",
        "                    elif split == 'test':\n",
        "                        test_dataset.append(data_tuple)\n",
        "                    elif split == 'valid':\n",
        "                        valid_dataset.append(data_tuple)\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar {img_path}: {e}\")\n",
        "\n",
        "    if not valid_dataset:\n",
        "        print(\"Aviso: Nenhum dado de validação encontrado. Dividindo o dataset de treino.\")\n",
        "        split_ratio = 0.9  # Padrão para split treino/validação\n",
        "        split_idx = int(len(train_dataset) * split_ratio)\n",
        "        valid_dataset = train_dataset[split_idx:]\n",
        "        train_dataset = train_dataset[:split_idx]\n",
        "\n",
        "    return train_dataset, test_dataset, valid_dataset\n",
        "\n",
        "def get_datasets(dset_name):\n",
        "    if dset_name == 'custom':\n",
        "        return get_datasets_from_folders('/content/gdrive/MyDrive/datasets/ships_simpleCNN')\n",
        "    else:\n",
        "        # Chamada original\n",
        "        return get_datasets_original(dset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86LXO9p0334P"
      },
      "outputs": [],
      "source": [
        "def get_datasets_original(dset_name):\n",
        "    dset_name = dset_name.lower()\n",
        "    print(f\"Loading dataset ({dset_name})\")\n",
        "    if dset_name == 'mnist':\n",
        "        train_dataset = dsets.MNIST(\n",
        "            root='./data',\n",
        "            train=True,\n",
        "            transform=transforms.ToTensor(),\n",
        "            download=True)\n",
        "        new_train_dataset = []\n",
        "        for d in train_dataset:\n",
        "            new_train_dataset.append((d[0].numpy().flatten(), d[1]))\n",
        "        train_dataset = new_train_dataset\n",
        "        test_dataset = dsets.MNIST(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            transform=transforms.ToTensor())\n",
        "        new_test_dataset = []\n",
        "        for d in test_dataset:\n",
        "            new_test_dataset.append((d[0].numpy().flatten(), d[1]))\n",
        "        test_dataset = new_test_dataset\n",
        "    else:\n",
        "        train_dataset, test_dataset = tabular_tools.get_dataset(dset_name)\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U27mr5xP385z"
      },
      "outputs": [],
      "source": [
        "def binarize_datasets(train_dataset, test_dataset, valid_dataset, bits_per_input):\n",
        "    \"\"\"\n",
        "    Binariza os datasets de treino, validação e teste.\n",
        "    \"\"\"\n",
        "    std_skews = [norm.ppf((i + 1) / (bits_per_input + 1)) for i in range(bits_per_input)]\n",
        "\n",
        "    print(\"Binarizando dataset de treino e validação\")\n",
        "    train_inputs = []\n",
        "    train_labels = []\n",
        "    for d in train_dataset:\n",
        "        train_inputs.append(d[0])\n",
        "        train_labels.append(d[1])\n",
        "    train_inputs = np.array(train_inputs)\n",
        "    train_labels = np.array(train_labels)\n",
        "    use_gaussian_encoding = True\n",
        "    if use_gaussian_encoding:\n",
        "        mean_inputs = train_inputs.mean(axis=0)\n",
        "        std_inputs = train_inputs.std(axis=0)\n",
        "        train_binarizations = [(train_inputs >= mean_inputs + (i * std_inputs)).astype(np.uint8) for i in std_skews]\n",
        "    else:\n",
        "        min_inputs = train_inputs.min(axis=0)\n",
        "        max_inputs = train_inputs.max(axis=0)\n",
        "        train_binarizations = [(train_inputs > min_inputs + (((i + 1) / (bits_per_input + 1)) * (max_inputs - min_inputs))).astype(np.uint8) for i in range(bits_per_input)]\n",
        "\n",
        "    # Codificação termômetro\n",
        "    train_inputs = np.concatenate(train_binarizations, axis=1)\n",
        "\n",
        "    if valid_dataset:\n",
        "        val_inputs = []\n",
        "        val_labels = []\n",
        "        for d in valid_dataset:\n",
        "            val_inputs.append(d[0])\n",
        "            val_labels.append(d[1])\n",
        "        val_inputs = np.array(val_inputs)\n",
        "        val_labels = np.array(val_labels)\n",
        "        val_binarizations = [(val_inputs >= mean_inputs + (i * std_inputs)).astype(np.uint8) for i in std_skews]\n",
        "        val_inputs = np.concatenate(val_binarizations, axis=1)\n",
        "    else:\n",
        "        val_inputs, val_labels = None, None\n",
        "\n",
        "    print(\"Binarizando dataset de teste\")\n",
        "    test_inputs = []\n",
        "    test_labels = []\n",
        "    for d in test_dataset:\n",
        "        test_inputs.append(d[0])\n",
        "        test_labels.append(d[1])\n",
        "    test_inputs = np.array(test_inputs)\n",
        "    test_labels = np.array(test_labels)\n",
        "    test_binarizations = [(test_inputs >= mean_inputs + (i * std_inputs)).astype(np.uint8) for i in std_skews]\n",
        "    test_inputs = np.concatenate(test_binarizations, axis=1)\n",
        "\n",
        "    return train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "865gsEiC4B_d"
      },
      "outputs": [],
      "source": [
        "def create_models(dset_name, unit_inputs, unit_entries, unit_hashes, bits_per_input, num_workers, save_prefix=\"model\"):\n",
        "    \"\"\"\n",
        "    Cria modelos para as configurações especificadas e salva os melhores resultados.\n",
        "    \"\"\"\n",
        "    # Carregar os datasets (agora incluindo validação)\n",
        "    train_dataset, test_dataset, valid_dataset = get_datasets(dset_name)\n",
        "\n",
        "    # Binarizar os datasets\n",
        "    datasets = binarize_datasets(train_dataset, test_dataset, valid_dataset, bits_per_input)\n",
        "\n",
        "    # Criar combinações de hiperparâmetros\n",
        "    prod = list(itertools.product(unit_inputs, unit_entries, unit_hashes))\n",
        "    configurations = [datasets + c for c in prod]\n",
        "\n",
        "    # Determinar o número de workers\n",
        "    if num_workers == -1:\n",
        "        num_workers = cpu_count()\n",
        "\n",
        "    print(f\"Lançando jobs para {len(configurations)} configurações usando {num_workers} workers\")\n",
        "    with Pool(num_workers) as p:\n",
        "        results = p.starmap(parameterized_run, configurations)\n",
        "\n",
        "    # Avaliar e imprimir os melhores resultados por número de entradas\n",
        "    for entries in unit_entries:\n",
        "        max_result = max([results[i][1] for i in range(len(results)) if configurations[i][7] == entries])\n",
        "        print(f\"Melhor com {entries} entradas: {max_result}\")\n",
        "\n",
        "    # Ordenar configurações por desempenho\n",
        "    configs_plus_results = [[configurations[i][6:9]] + list(results[i]) for i in range(len(results))]\n",
        "    configs_plus_results.sort(reverse=True, key=lambda x: x[2])  # Ordena pelo score\n",
        "\n",
        "    for i in configs_plus_results:\n",
        "        print(f\"{i[0]}: {i[2]} ({i[2] / len(datasets[4])})\")  # Normaliza pelo tamanho do teste\n",
        "\n",
        "    # Criar diretório para salvar os modelos\n",
        "    os.makedirs(os.path.dirname(f\"./models/{dset_name}/{save_prefix}\"), exist_ok=True)\n",
        "\n",
        "    # Salvar modelos no disco\n",
        "    for idx, result in enumerate(results):\n",
        "        model = result[0]\n",
        "        model_inputs, model_entries, model_hashes = configurations[idx][6:9]\n",
        "        save_model(model, datasets[0][0].size // bits_per_input,\n",
        "                   f\"./models/{dset_name}/{save_prefix}_{model_inputs}input_{model_entries}entry_{model_hashes}hash_{bits_per_input}bpi.pickle.lzma\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbbeLM_I79W8"
      },
      "outputs": [],
      "source": [
        "def save_model(model, num_inputs, fname):\n",
        "    model.binarize()\n",
        "    model_info = {\n",
        "        \"num_inputs\": num_inputs,\n",
        "        \"num_classes\": len(model.discriminators),\n",
        "        \"bits_per_input\": len(model.input_order) // num_inputs,\n",
        "        \"num_filter_inputs\": model.discriminators[0].filters[0].num_inputs,\n",
        "        \"num_filter_entries\": model.discriminators[0].filters[0].num_entries,\n",
        "        \"num_filter_hashes\": model.discriminators[0].filters[0].num_hashes,\\\n",
        "        \"hash_values\": model.discriminators[0].filters[0].hash_values\n",
        "    }\n",
        "    state_dict = {\n",
        "        \"info\": model_info,\n",
        "        \"model\": model\n",
        "    }\n",
        "\n",
        "    with lzma.open(fname, \"wb\") as f:\n",
        "        pickle.dump(state_dict, f)\n",
        "\n",
        "def read_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Train BTHOWeN models for a dataset with specified hyperparameter sweep\")\n",
        "    parser.add_argument(\"dset_name\", help=\"Name of dataset to use\")\n",
        "    parser.add_argument(\"--filter_inputs\", nargs=\"+\", required=True, type=int,\\\n",
        "            help=\"Number of inputs to each Bloom filter (accepts multiple values)\")\n",
        "    parser.add_argument(\"--filter_entries\", nargs=\"+\", required=True, type=int,\\\n",
        "            help=\"Number of entries in each Bloom filter (accepts multiple values; must be powers of 2)\")\n",
        "    parser.add_argument(\"--filter_hashes\", nargs=\"+\", required=True, type=int,\\\n",
        "            help=\"Number of distinct hash functions for each Bloom filter (accepts multiple values)\")\n",
        "    parser.add_argument(\"--bits_per_input\", nargs=\"+\", required=True, type=int,\\\n",
        "            help=\"Number of thermometer encoding bits for each input in the dataset (accepts multiple values)\")\n",
        "    parser.add_argument(\"--save_prefix\", default=\"model\", help=\"Partial path/fname to prepend to each output file\")\n",
        "    parser.add_argument(\"--num_workers\", default=-1, type=int, help=\"Number of processes to run in parallel; defaults to number of logical CPUs\")\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    args = read_arguments()\n",
        "\n",
        "    for bpi in args.bits_per_input:\n",
        "        print(f\"Do runs with {bpi} bit(s) per input\")\n",
        "        create_models(\n",
        "            args.dset_name, args.filter_inputs, args.filter_entries, args.filter_hashes,\n",
        "            bpi, args.num_workers, args.save_prefix)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Vk95Sw8ln5"
      },
      "source": [
        "###**Testando execução**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z41jyanN9Dg-"
      },
      "outputs": [],
      "source": [
        "!chmod 777 /content/BTHOWeN/software_model/train_swept_models_alt.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0bQW2Yw8k4Y",
        "outputId": "c4e53e04-a877-4615-d46b-a64de266fa0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do runs with 4 bit(s) per input\n",
            "Binarizando dataset de treino e validação\n",
            "Binarizando dataset de teste\n",
            "Lançando jobs para 1 configurações usando 2 workers\n",
            "Training model\n",
            "Maximum possible bleach value is 65\n",
            "With bleaching=16, accuracy=23.55%; precision=0.2710; recall=0.2355\n",
            "Ties=295/484 (60.9504%)\n",
            "With bleaching=32, accuracy=22.93%; precision=0.3656; recall=0.2293\n",
            "Ties=399/484 (82.438%)\n",
            "With bleaching=48, accuracy=18.39%; precision=0.1469; recall=0.1839\n",
            "Ties=460/484 (95.0413%)\n",
            "With bleaching=8, accuracy=29.96%; precision=0.4375; recall=0.2996\n",
            "Ties=165/484 (34.0909%)\n",
            "With bleaching=24, accuracy=25.41%; precision=0.2494; recall=0.2541\n",
            "Ties=348/484 (71.9008%)\n",
            "With bleaching=4, accuracy=42.77%; precision=0.4767; recall=0.4277\n",
            "Ties=64/484 (13.2231%)\n",
            "With bleaching=12, accuracy=24.17%; precision=0.2367; recall=0.2417\n",
            "Ties=238/484 (49.1736%)\n",
            "With bleaching=2, accuracy=53.72%; precision=0.5862; recall=0.5372\n",
            "Ties=14/484 (2.8926%)\n",
            "With bleaching=6, accuracy=37.60%; precision=0.4468; recall=0.3760\n",
            "Ties=120/484 (24.7934%)\n",
            "With bleaching=1, accuracy=54.55%; precision=0.5962; recall=0.5455\n",
            "Ties=8/484 (1.6529%)\n",
            "With bleaching=3, accuracy=47.52%; precision=0.5118; recall=0.4752\n",
            "Ties=36/484 (7.438%)\n",
            "Best bleach: 1; inputs/entries/hashes = 29,8192,3\n",
            "Testing model\n",
            "With bleaching=1, accuracy=44.36%; precision=0.4470; recall=0.4436\n",
            "Ties=9/523 (1.7208%)\n",
            "Test Results - Accuracy: 44.36%, Precision: 0.4470, Recall: 0.4436\n",
            "Melhor com 8192 entradas: 0.4435946462715105\n",
            "(29, 8192, 3): 0.4435946462715105 (0.000848173319830804)\n"
          ]
        }
      ],
      "source": [
        "!/content/BTHOWeN/software_model/train_swept_models_alt.py classnavios --filter_inputs 29 --filter_entries 8192 --filter_hashes 3 --bits_per_input 4 --num_workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-gOe6r8JFRJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bA-zmHLMo4ZR"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}